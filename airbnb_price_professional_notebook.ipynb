{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9b965a",
   "metadata": {},
   "source": [
    "\n",
    "# Airbnb Price Prediction — Professional (Internship-ready)\n",
    "**Style:** Data Scientist / Machine Learning Engineer (Professional)\n",
    "**Dataset:** https://www.kaggle.com/datasets/stevezhenghp/airbnb-price-prediction\n",
    "\n",
    "**What this notebook contains (high level)**\n",
    "- Clear problem statement and success metrics\n",
    "- Data ingestion (Kaggle API + manual upload instructions)\n",
    "- Exploratory Data Analysis (EDA) with visualizations\n",
    "- Robust preprocessing & advanced feature engineering:\n",
    "  - Price cleaning + log-transform target\n",
    "  - Amenities parsing and amenity scoring\n",
    "  - Bathroom/bedroom parsing, capacity features\n",
    "  - Geospatial features (latitude/longitude → distance to city center, clustering neighborhoods)\n",
    "  - Outlier handling and missing value strategy\n",
    "- Modeling & Evaluation:\n",
    "  - Train/test split + K-Fold CV\n",
    "  - Model comparison: RandomForest, XGBoost, ExtraTrees, GradientBoosting\n",
    "  - Hyperparameter tuning (RandomizedSearchCV for speed)\n",
    "  - Cross-validated metrics (RMSE, MAE, R2) and final model selection\n",
    "- Explainability & Interpretation:\n",
    "  - Feature importance + SHAP summary plots\n",
    "- Deliverables:\n",
    "  - Save final model, processed CSV for dashboards, and helper `predict_price()` function\n",
    "  - Instructions to create a PPT and PDF report from results (cells included)\n",
    "- Notes: Run in Google Colab. Some cells install packages (xgboost, shap, python-pptx) — allow them to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3f5b6",
   "metadata": {},
   "source": [
    "## 1) Download dataset (Kaggle API) or Upload CSV\n",
    "Follow one of the two options:\n",
    "\n",
    "**A) Upload manually**: Drag & drop `listings.csv` into Colab `/content` (left Files pane).\n",
    "\n",
    "**B) Kaggle API**: Upload your `kaggle.json` to `/content` and run the cell below to download and unzip the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Download dataset using Kaggle API (if you uploaded kaggle.json to /content)\n",
    "!pip install -q kaggle\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
    "# After you upload kaggle.json to /content, run:\n",
    "!kaggle datasets download -d stevezhenghp/airbnb-price-prediction -p /content --unzip\n",
    "!ls -lh /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75757929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install extra packages not always present in Colab\n",
    "!pip install -q xgboost shap python-pptx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard imports\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib, os, json\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "print('imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csvs = [f for f in os.listdir('/content') if f.lower().endswith('.csv')]\n",
    "print('CSV files found:', csvs)\n",
    "# default attempt\n",
    "fn = '/content/listings.csv' if '/content/listings.csv' in ['/content/'+c for c in csvs] else ('/content/' + csvs[0] if csvs else None)\n",
    "if fn is None:\n",
    "    raise FileNotFoundError('No CSV found. Upload listings.csv or use Kaggle API cell.')\n",
    "df = pd.read_csv(fn, low_memory=False)\n",
    "print('Loaded', fn, 'shape=', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- EDA (quick) ---\n",
    "print('Columns:', len(df.columns))\n",
    "print(df.select_dtypes(include=['object']).columns[:30])\n",
    "print('\\nPrice column candidates:')\n",
    "candidates = [c for c in df.columns if 'price' in c.lower()]\n",
    "print(candidates)\n",
    "# Show basic distribution if price exists\n",
    "price_col = None\n",
    "for c in df.columns:\n",
    "    if c.lower()=='price' or 'price' in c.lower():\n",
    "        price_col = c; break\n",
    "print('Using price column:', price_col)\n",
    "if price_col:\n",
    "    df[price_col] = df[price_col].astype(str)\n",
    "    # Clean preview 10\n",
    "    print(df[price_col].head(10))\n",
    "    # quick stats (after cleaning step below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Preprocessing & Feature Engineering\n",
    "# ----------------------\n",
    "\n",
    "import re\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Helper: clean price to float\n",
    "def clean_price_col(s):\n",
    "    try:\n",
    "        if pd.isna(s): return np.nan\n",
    "        s = str(s)\n",
    "        s = re.sub(r'[^0-9.]', '', s)\n",
    "        if s=='' or s=='.': return np.nan\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Identify likely column names with heuristics\n",
    "cols = df.columns.tolist()\n",
    "def find_col(options):\n",
    "    for o in options:\n",
    "        for c in cols:\n",
    "            if o.lower()==c.lower(): return c\n",
    "    return None\n",
    "\n",
    "col_price = find_col(['price','Price','price_per_night','price_night'])\n",
    "col_lat = find_col(['latitude','lat'])\n",
    "col_lng = find_col(['longitude','lng','long'])\n",
    "col_amen = find_col(['amenities','Amenities'])\n",
    "col_room = find_col(['room_type','room type','property_type','property type','property_type'])\n",
    "col_bedrooms = find_col(['bedrooms'])\n",
    "col_bath = find_col(['bathrooms','bathrooms_text'])\n",
    "col_accom = find_col(['accommodates','guests'])\n",
    "\n",
    "print('Mapped:', col_price, col_lat, col_lng, col_amen, col_room, col_bedrooms, col_bath, col_accom)\n",
    "\n",
    "# Clean price and create target\n",
    "df['price_clean'] = df[col_price].apply(clean_price_col)\n",
    "print('price_clean na:', df['price_clean'].isna().sum())\n",
    "\n",
    "# Basic numeric features\n",
    "for c in [col_bedrooms, col_bath, col_accom]:\n",
    "    if c and c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Amenities: count and create amenity score\n",
    "if col_amen and col_amen in df.columns:\n",
    "    # remove braces and split; dataset often stores as string like \"{'Wifi', 'Kitchen'}\"\n",
    "    df['amen_list'] = df[col_amen].fillna('[]').astype(str).apply(lambda x: re.findall(r\"'([^']+)'|\\\"([^\\\"]+)\\\"|([A-Za-z0-9 _+-]+)\", x))\n",
    "    # amen_list above is complex; simpler count by commas if that fails\n",
    "    def amen_count_raw(x):\n",
    "        try:\n",
    "            s = str(x)\n",
    "            s = s.strip('{}[] ')\n",
    "            if s=='' or s.lower()=='nan': return 0\n",
    "            # split by comma not within quotes approx\n",
    "            return len([a for a in re.split(r',\\s*(?![^\\(]*\\))', s) if a.strip()!=''])\n",
    "        except:\n",
    "            return 0\n",
    "    df['amenities_count'] = df[col_amen].fillna('').apply(amen_count_raw)\n",
    "    amenities_lower = df[col_amen].fillna('').str.lower()\n",
    "    # create flags for common amenities\n",
    "    for a in ['wifi','kitchen','heater','heating','washer','dryer','parking','parking space','air conditioning','air conditioning','ac']:\n",
    "        colname = 'amen_' + re.sub(r'\\W+','_',a)\n",
    "        df[colname] = amenities_lower.str.contains(a, na=False).astype(int)\n",
    "else:\n",
    "    df['amenities_count'] = 0\n",
    "\n",
    "# Bathrooms: sometimes in text \"1 bath\", \"1.5 shared baths\"\n",
    "if col_bath and col_bath in df.columns:\n",
    "    df['bath_numeric'] = pd.to_numeric(df[col_bath].astype(str).str.extract(r'([0-9\\.]+)')[0], errors='coerce')\n",
    "\n",
    "# Latitude/longitude distance to centroid (approx center) if available\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6371 * c\n",
    "    return km\n",
    "\n",
    "if col_lat and col_lat in df.columns and col_lng and col_lng in df.columns:\n",
    "    # compute city centroid as median of available coords\n",
    "    center_lat = df[col_lat].median()\n",
    "    center_lng = df[col_lng].median()\n",
    "    df['dist_to_center_km'] = df.apply(lambda row: haversine(row[col_lng], row[col_lat], center_lng, center_lat) if pd.notna(row[col_lat]) and pd.notna(row[col_lng]) else np.nan, axis=1)\n",
    "else:\n",
    "    df['dist_to_center_km'] = np.nan\n",
    "\n",
    "# Create final feature list candidates\n",
    "feature_candidates = ['amenities_count','dist_to_center_km','bath_numeric']\n",
    "for c in df.columns:\n",
    "    if c.startswith('amen_'): feature_candidates.append(c)\n",
    "if col_bedrooms and col_bedrooms in df.columns: feature_candidates.append(col_bedrooms)\n",
    "if col_accom and col_accom in df.columns: feature_candidates.append(col_accom)\n",
    "if col_room and col_room in df.columns: feature_candidates.append(col_room)\n",
    "# drop na target rows\n",
    "df_model = df.copy()\n",
    "df_model = df_model.dropna(subset=['price_clean']).reset_index(drop=True)\n",
    "print('df_model shape after dropping NA price:', df_model.shape)\n",
    "\n",
    "# Target log-transform\n",
    "df_model['log_price'] = np.log1p(df_model['price_clean'])\n",
    "\n",
    "# Save processed snapshot for inspection\n",
    "df_model.sample(3).T.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Modeling: comparison + tuning with K-Fold CV\n",
    "# ----------------------\n",
    "\n",
    "# Select features for modeling (mix numeric + categorical)\n",
    "numeric_features = [c for c in feature_candidates if c in df_model.columns and df_model[c].dtype in [np.float64, np.int64, 'float64','int64']]\n",
    "categorical_features = [col_room] if (col_room and col_room in df_model.columns) else []\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "\n",
    "X = df_model[numeric_features + categorical_features].copy()\n",
    "y = df_model['log_price']  # model on log target\n",
    "\n",
    "# Fill NA for numeric\n",
    "X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())\n",
    "\n",
    "# Simple train-test split (holdout)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "# Preprocessor: scale numeric, one-hot categorical\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    'GradientBoost': GradientBoostingRegressor(n_estimators=200, random_state=42),\n",
    "}\n",
    "\n",
    "# XGBoost optionally\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    models['XGBoost'] = xgb.XGBRegressor(n_estimators=200, random_state=42, verbosity=0, n_jobs=-1, objective='reg:squarederror')\n",
    "except Exception as e:\n",
    "    print('xgboost not available:', e)\n",
    "\n",
    "# Function to evaluate model with cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('pre', preprocessor), ('model', model)])\n",
    "    # 5-fold CV on training set (neg MSE)\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    results.append((name, -scores.mean(), -scores.std()))\n",
    "    print(f'{name} CV RMSE: {-scores.mean():.4f} ± {-scores.std():.4f}')\n",
    "\n",
    "# Fit each model on full train and evaluate on test\n",
    "test_results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('pre', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred_log = pipe.predict(X_test)\n",
    "    # invert log transform\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_true = np.expm1(y_test)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    test_results.append((name, rmse, mae, r2))\n",
    "    # save model file\n",
    "    joblib.dump(pipe, f'/content/{name}_pipeline.joblib')\n",
    "    print(f'{name} Test RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.4f} -- saved to /content/{name}_pipeline.joblib')\n",
    "\n",
    "# Present test results as dataframe\n",
    "res_df = pd.DataFrame(test_results, columns=['model','RMSE','MAE','R2']).sort_values('RMSE')\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Hyperparameter tuning with RandomizedSearchCV on the best model (choose top from res_df)\n",
    "# ----------------------\n",
    "res_df = res_df.reset_index(drop=True)\n",
    "best_model_name = res_df.loc[0,'model']\n",
    "print('Best model to tune:', best_model_name)\n",
    "if best_model_name == 'RandomForest':\n",
    "    param_dist = {\n",
    "        'model__n_estimators': [100,200,400],\n",
    "        'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2,5,10],\n",
    "        'model__min_samples_leaf': [1,2,4]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_dist = {\n",
    "        'model__n_estimators': [100,200,400],\n",
    "        'model__learning_rate': [0.01,0.05,0.1],\n",
    "        'model__max_depth': [3,6,10],\n",
    "        'model__subsample': [0.6,0.8,1.0]\n",
    "    }\n",
    "else:\n",
    "    # generic for tree models\n",
    "    param_dist = {\n",
    "        'model__n_estimators': [100,200,400],\n",
    "        'model__max_depth': [None, 6, 10, 20],\n",
    "        'model__min_samples_split': [2,5,10]\n",
    "    }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# load the pipeline for best model\n",
    "best_pipe = joblib.load(f'/content/{best_model_name}_pipeline.joblib')\n",
    "rs = RandomizedSearchCV(best_pipe, param_distributions=param_dist, n_iter=20, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=42, verbose=1)\n",
    "rs.fit(X_train, y_train)\n",
    "print('Best params:', rs.best_params_)\n",
    "print('Best CV score (neg RMSE):', rs.best_score_)\n",
    "# evaluate on test\n",
    "y_pred_log = rs.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "print('Tuned Test RMSE:', mean_squared_error(y_true, y_pred, squared=False))\n",
    "joblib.dump(rs.best_estimator_, f'/content/{best_model_name}_tuned_pipeline.joblib')\n",
    "print('Saved tuned model to', f'/content/{best_model_name}_tuned_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# SHAP explainability for the tuned model (or best estimator)\n",
    "# ----------------------\n",
    "try:\n",
    "    import shap\n",
    "    tuned_path = f'/content/{best_model_name}_tuned_pipeline.joblib'\n",
    "    if os.path.exists(tuned_path):\n",
    "        tuned = joblib.load(tuned_path)\n",
    "    else:\n",
    "        tuned = joblib.load(f'/content/{best_model_name}_pipeline.joblib')\n",
    "    # get preprocessed X_train for shap sampling\n",
    "    pre = tuned.named_steps['pre']\n",
    "    model = tuned.named_steps['model']\n",
    "    X_pre = pre.transform(X_train)\n",
    "    # shap depends on model type; use TreeExplainer if supported\n",
    "    explainer = shap.Explainer(model)\n",
    "    # sample small subset for speed\n",
    "    sample = X_train.sample(min(100, len(X_train)), random_state=42)\n",
    "    shap_values = explainer(pre.transform(sample))\n",
    "    print('Plotting SHAP summary (may open in notebook)')\n",
    "    shap.summary_plot(shap_values, features=pre.transform(sample), feature_names=(pre.named_transformers_['num'].named_steps['scaler'].get_feature_names_out(numeric_features) if hasattr(pre.named_transformers_['num'].named_steps['scaler'],'get_feature_names_out') else numeric_features))\n",
    "except Exception as e:\n",
    "    print('SHAP step skipped or failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db70913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Helper: load final model and helper predict function\n",
    "# ----------------------\n",
    "# Use tuned model if exists, else best model\n",
    "final_path = None\n",
    "if os.path.exists(f'/content/{best_model_name}_tuned_pipeline.joblib'):\n",
    "    final_path = f'/content/{best_model_name}_tuned_pipeline.joblib'\n",
    "else:\n",
    "    final_path = f'/content/{best_model_name}_pipeline.joblib'\n",
    "final_model = joblib.load(final_path)\n",
    "print('Final model loaded from', final_path)\n",
    "\n",
    "def predict_price(row_dict):\n",
    "    # row_dict should contain keys for numeric_features + categorical_features\n",
    "    x = pd.DataFrame([row_dict])\n",
    "    # ensure numeric features present\n",
    "    for f in numeric_features:\n",
    "        if f not in x.columns: x[f] = np.nan\n",
    "    for f in categorical_features:\n",
    "        if f not in x.columns: x[f] = None\n",
    "    x[numeric_features] = x[numeric_features].fillna(df_model[numeric_features].median())\n",
    "    pred_log = final_model.predict(x)[0]\n",
    "    return float(np.expm1(pred_log))\n",
    "\n",
    "# Example usage (modify values):\n",
    "example = {f: float(df_model[f].median()) if f in numeric_features else (df_model[f].mode()[0] if f in categorical_features else None) for f in numeric_features + categorical_features}\n",
    "print('Example input:', example)\n",
    "print('Predicted price for example (USD):', predict_price(example))\n",
    "\n",
    "# Export processed CSV for dashboards\n",
    "processed_path = '/content/airbnb_processed_for_dashboard.csv'\n",
    "df_model.to_csv(processed_path, index=False)\n",
    "print('Saved processed dataset to', processed_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb08af7",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables included\n",
    "- `*_pipeline.joblib` files for each candidate model (in /content)  \n",
    "- `*_tuned_pipeline.joblib` for the tuned best model (if tuning ran)  \n",
    "- `airbnb_processed_for_dashboard.csv` — exported processed dataset for Power BI/Tableau  \n",
    "- Notebook (`.ipynb`) — this file (downloadable)  \n",
    "\n",
    "### Next optional deliverables I can add for you:\n",
    "- Automated **Project PPT** (slides) and **PDF documentation** generated from notebook results.\n",
    "- Readme and `requirements.txt` for GitHub repo.\n",
    "- A short script (`present_me.txt`) with bullet points for your interview presentation.\n",
    "\n",
    "Reply **\"Add PPT & PDF\"** if you want me to also generate PPT and PDF files now (I'll include a polished 8–10 slide PPT and a one-page PDF summary).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
